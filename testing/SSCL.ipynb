{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Existing File:  FullPickleData.txt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# For the use of padding\n",
    "from torch.nn.utils.rnn import pad_sequence, pad_packed_sequence, pack_padded_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as Data\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from copy import deepcopy\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import nltk\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "import itertools\n",
    "import gensim\n",
    "import os\n",
    "from nltk.corpus import wordnet as wn  # For WordNet\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "'''\n",
    "TODO:\n",
    "\n",
    "1. Attn Transformer\n",
    "'''\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class args(object):\n",
    "\n",
    "    # Data\n",
    "    \n",
    "    usingWeightRandomSampling = True\n",
    "    \n",
    "    full_data = True\n",
    "    if full_data: \n",
    "        pickle_name = \"FullPickleData.txt\"\n",
    "    else:\n",
    "        pickle_name = \"10thPickleData.txt\"\n",
    "    dataset_path = \"\"  # load a dataset and setting\n",
    "    vocab_size = 500\n",
    "\n",
    "    # Arch\n",
    "\n",
    "    usingPretrainedEmbedding = False\n",
    "    if usingPretrainedEmbedding:\n",
    "        embedding_dim = 300\n",
    "    else:\n",
    "        embedding_dim = 512\n",
    "\n",
    "    RNN_hidden = 256\n",
    "    num_CNN_filter = 256\n",
    "    CNN_kernel_size = 5\n",
    "    LSTM_dropout = 0.1\n",
    "\n",
    "    # Training params\n",
    "\n",
    "    confusion_matrics = []\n",
    "    \n",
    "    batch_size = 64\n",
    "    L2 = 0.1\n",
    "    threshold = 0.75\n",
    "    lr = 0.002\n",
    "    n_epoch = 50\n",
    "\n",
    "    # If using Adam\n",
    "    adam_beta1 = 0.9\n",
    "    adam_beta2 = 0.999\n",
    "    adam_weight_decay = 0.01\n",
    "\n",
    "    # Logging the Training\n",
    "    val_freq = 200\n",
    "    val_steps = 3\n",
    "    log_freq = 10\n",
    "    model_save_freq = 1\n",
    "    model_name = 'SSCL_4'\n",
    "    model_path = './' + model_name + '/Model/'\n",
    "    log_path = './' + model_name + '/Log/'\n",
    "\n",
    "\n",
    "# Create the path for saving model and the log\n",
    "if not os.path.exists(args.model_path):\n",
    "    os.makedirs(args.model_path)\n",
    "\n",
    "if not os.path.exists(args.log_path):\n",
    "    os.makedirs(args.log_path)\n",
    "\n",
    "\n",
    "class Constants():\n",
    "    ''' The Constants for the text '''\n",
    "    PAD = 0\n",
    "    UNK = 1\n",
    "    SOS = 2\n",
    "    EOS = 3\n",
    "\n",
    "    PAD_WORD = '<PAD>'\n",
    "    UNK_WORD = '<UNK>'\n",
    "    SOS_WORD = '<SOS>'\n",
    "    EOS_WORD = '<EOS>'\n",
    "\n",
    "\n",
    "specialTokens = {\n",
    "    Constants.PAD_WORD: Constants.PAD,\n",
    "    Constants.UNK_WORD: Constants.UNK,\n",
    "    Constants.SOS_WORD: Constants.SOS,\n",
    "    Constants.EOS_WORD: Constants.EOS,\n",
    "}\n",
    "\n",
    "specialTokenList = [Constants.PAD_WORD,\n",
    "                    Constants.UNK_WORD,\n",
    "                    Constants.SOS_WORD,\n",
    "                    Constants.EOS_WORD]\n",
    "\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "# model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "#     './GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "# word2vector = torch.FloatTensor(model.vectors)\n",
    "\n",
    "\n",
    "class SSCL(nn.Module):\n",
    "\n",
    "    ''' The Model from paper '''\n",
    "\n",
    "    def __init__(self,):\n",
    "        super(SSCL, self).__init__()\n",
    "\n",
    "        if args.usingPretrainedEmbedding:\n",
    "            self.embed = nn.Embedding.from_pretrained(word2vector)\n",
    "        else:\n",
    "            self.embed = nn.Embedding(\n",
    "                args.vocab_size, args.embedding_dim, Constants.PAD)\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(args.embedding_dim, args.num_CNN_filter,\n",
    "                      args.CNN_kernel_size, 1, 2),\n",
    "            nn.BatchNorm1d(args.num_CNN_filter),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv1d(args.num_CNN_filter, args.num_CNN_filter*2,\n",
    "              args.CNN_kernel_size, 1, 2),\n",
    "            nn.BatchNorm1d(args.num_CNN_filter*2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv1d(args.num_CNN_filter*2, args.num_CNN_filter,\n",
    "              args.CNN_kernel_size, 1, 2),\n",
    "            nn.BatchNorm1d(args.num_CNN_filter),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.rnn = nn.LSTM(args.num_CNN_filter, args.RNN_hidden,\n",
    "                           batch_first=True, dropout=args.LSTM_dropout, num_layers = 3)\n",
    "\n",
    "        self.out_net = nn.Sequential(\n",
    "            nn.Linear(args.RNN_hidden, 1),\n",
    "        )\n",
    "\n",
    "        self.h0 = nn.Parameter(torch.randn(1, args.RNN_hidden))\n",
    "        self.c0 = nn.Parameter(torch.randn(1, args.RNN_hidden))\n",
    "\n",
    "#         self.apply(self.weight_init)\n",
    "\n",
    "    def forward(self, input, lengths=None):\n",
    "\n",
    "        B = input.size(0)\n",
    "\n",
    "        emb_out = self.embed(input).transpose(1, 2)\n",
    "\n",
    "        out = self.cnn(emb_out).transpose(1, 2)\n",
    "\n",
    "        if not lengths is None:\n",
    "            out = pack_padded_sequence(out, lengths, batch_first=True)\n",
    "#             out, hidden = self.rnn(\n",
    "#                 out, (self.h0.repeat(1, B, 1), self.c0.repeat(1, B, 1)))\n",
    "            out, hidden = self.rnn(out)\n",
    "            out = pad_packed_sequence(out, batch_first=True)[0][:, -1, :]\n",
    "        else:\n",
    "            #             out = self.rnn(out,(self.h0.repeat(1,B,1), self.c0.repeat(1,B,1)))[0][:, -1, :]\n",
    "            out = self.rnn(out)[0][:, -1, :]\n",
    "\n",
    "        out = self.out_net(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def weight_init(self, m):\n",
    "\n",
    "        if type(m) in [nn.Conv2d, nn.ConvTranspose2d, nn.Linear]:\n",
    "            nn.init.kaiming_normal_(m.weight, 0.2, nonlinearity='leaky_relu')\n",
    "        elif type(m) in [nn.LSTM]:\n",
    "            for name, value in m.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_normal_(value.data)\n",
    "                if 'bias'in name:\n",
    "                    value.data.normal_()\n",
    "\n",
    "# We can put all the lost function and the optimization into the args\n",
    "\n",
    "class Trainer(nn.Module):\n",
    "\n",
    "    def __init__(self,):\n",
    "        super(Trainer, self).__init__()\n",
    "        \n",
    "        self.SSCL = SSCL()\n",
    "        self.optim = optim.Adam(\n",
    "            self.SSCL.parameters(), lr=args.lr, weight_decay=args.adam_weight_decay)\n",
    "        \n",
    "#         if args.usingWeightRandomSampling:\n",
    "#             pos_weight = None\n",
    "#         else:\n",
    "        numberOfSpammer = training_dataset.tensors[2].sum()\n",
    "        numberOfNoSpammer = len(training_dataset)-training_dataset.tensors[2].sum()\n",
    "        pos_weight=torch.tensor(numberOfNoSpammer/numberOfSpammer)\n",
    "\n",
    "        self.Loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        self.hist = defaultdict(list)\n",
    "\n",
    "    def forward(self, input, label):\n",
    "\n",
    "        if type(input) is tuple:\n",
    "            input, lengths = input\n",
    "        else:\n",
    "            lengths = None\n",
    "\n",
    "        self.pred = self.SSCL(input, lengths)\n",
    "\n",
    "        loss = self.Loss(self.pred.squeeze(1), label)\n",
    "\n",
    "        accuracy = torch.mean(\n",
    "            ((self.pred > args.threshold) == label.byte()).float())\n",
    "        \n",
    "        args.confusion_matrics.insert(0, confusion_matrix(label.cpu().numpy(), (self.pred > args.threshold).cpu().numpy()))\n",
    "        \n",
    "        args.confusion_matrics = args.confusion_matrics[:10]\n",
    "\n",
    "        return loss, accuracy\n",
    "\n",
    "    def train_step(self, input, label):\n",
    "\n",
    "        self.optim.zero_grad()\n",
    "\n",
    "        self.loss, self.accuracy = self.forward(input, label)\n",
    "\n",
    "        self.hist[\"Temp_Train_Loss\"].append(self.loss.item())\n",
    "        self.hist[\"Temp_Train_Accuracy\"].append(self.accuracy.item())\n",
    "        self.hist[\"Train_Loss\"].append(self.loss.item())\n",
    "        self.hist[\"Train_Accuracy\"].append(self.accuracy.item())\n",
    "\n",
    "        self.loss.backward()\n",
    "        self.optim.step()\n",
    "\n",
    "    def test_step(self, input, label, validation=True):\n",
    "\n",
    "        # Not Updating the weight\n",
    "\n",
    "        self.loss, self.accuracy = self.forward(input, label)\n",
    "\n",
    "        if validation:\n",
    "            self.hist[\"Temp_Val_Loss\"].append(self.loss.item())\n",
    "            self.hist[\"Temp_Val_Accuracy\"].append(self.accuracy.item())\n",
    "            self.hist[\"Val_Loss\"].append(self.loss.item())\n",
    "            self.hist[\"Val_Accuracy\"].append(self.accuracy.item())\n",
    "        else:\n",
    "            self.hist[\"Temp_Test_Loss\"].append(self.loss.item())\n",
    "            self.hist[\"Temp_Test_Accuracy\"].append(self.accuracy.item())\n",
    "            self.hist[\"Test_Loss\"].append(self.loss.item())\n",
    "            self.hist[\"Test_Accuracy\"].append(self.accuracy.item())\n",
    "\n",
    "    def calculateAverage(self,):\n",
    "\n",
    "        temp_keys = deepcopy(list(trainer.hist.keys()))\n",
    "        for name in temp_keys:\n",
    "            if 'Temp' in name:\n",
    "                self.hist[\"Average\" + name[4:]\n",
    "                          ].append(sum(self.hist[name])/len(self.hist[name]))\n",
    "                self.hist[name] = []\n",
    "\n",
    "    def plot_train_hist(self, step):\n",
    "        fig = plt.figure(figsize=(20, 10))\n",
    "        num_loss = 2\n",
    "        i = 0\n",
    "        for name in self.hist.keys():\n",
    "            if 'Train' in name and not \"Temp\" in name and not \"Average\" in name:\n",
    "                i += 1\n",
    "                fig.add_subplot(num_loss, 1, i)\n",
    "                plt.plot(self.hist[name], label=name)\n",
    "                plt.xlabel('Number of Steps', fontsize=15)\n",
    "                plt.ylabel(name, fontsize=15)\n",
    "                plt.title(name, fontsize=30, fontweight=\"bold\")\n",
    "                plt.legend(loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        fig.savefig(args.log_path+\"Train_Loss&Acc_Hist\"+str(step)+\".png\")\n",
    "\n",
    "    def plot_all(self, step=None):\n",
    "        \n",
    "        fig = plt.figure(figsize=(20, 10))\n",
    "        for name in self.hist.keys():\n",
    "            if \"Average\" in name:\n",
    "                if 'Loss' in name:\n",
    "                    plt.subplot(211)\n",
    "                    plt.plot(self.hist[name], marker='o', label=name)\n",
    "                    plt.ylabel('Loss', fontsize=15)\n",
    "                    plt.xlabel('Number of epochs', fontsize=15)\n",
    "                    plt.title('Loss', fontsize=20, fontweight=\"bold\")\n",
    "                    plt.legend(loc='upper left')\n",
    "                if \"Accuracy\" in name:\n",
    "                    plt.subplot(212)\n",
    "                    plt.plot(self.hist[name], marker='o', label=name)\n",
    "                    plt.ylabel('Accuracy', fontsize=15)\n",
    "                    plt.xlabel('Number of epochs', fontsize=15)\n",
    "                    plt.title('Accuracy', fontsize=20, fontweight=\"bold\")\n",
    "                    plt.legend(loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        if step is not None:\n",
    "            fig.savefig(args.log_path +\"All_Hist\"+str(step)+\".png\")\n",
    "\n",
    "    def model_save(self, step):\n",
    "\n",
    "        path = args.model_path + args.model_name+'_Step_' + str(step) + '.pth'\n",
    "        torch.save({args.model_name: self.state_dict()}, path)\n",
    "        print('Model Saved')\n",
    "\n",
    "    def load_step_dict(self, step):\n",
    "\n",
    "        path = args.model_path + args.model_name + \\\n",
    "            '_Step_' + str(step) + '.pth'\n",
    "        self.load_state_dict(torch.load(\n",
    "            path, map_location=lambda storage, loc: storage)[args.model_name])\n",
    "        print('Model Loaded')\n",
    "\n",
    "    def num_all_params(self,):\n",
    "        return sum([param.nelement() for param in self.parameters()])\n",
    "\n",
    "if not os.path.isfile(args.pickle_name):\n",
    "\n",
    "    def matchingURL(match):\n",
    "        args.match = match\n",
    "        try:\n",
    "            return urlparse(match.group()).netloc or match.group().split(\"/\")[0]\n",
    "        except:\n",
    "    #         print(\"The matching URL: \" ,match)\n",
    "    #         print(\"The Extracted webAdd: \",match.group().split(\"/\")[0])\n",
    "            return match.group().split(\"/\")[0]\n",
    "\n",
    "    def preprocessingInputDataForLoop(input):\n",
    "\n",
    "        allText = [i for i in input]\n",
    "\n",
    "        preprocessedText = []\n",
    "\n",
    "        for sentence in allText:\n",
    "\n",
    "            out_ = re.sub(r\"http\\S+|www.\\S+\",matchingURL, sentence)\n",
    "\n",
    "            out_ = re.sub(r'\\d+', '', out_)\n",
    "\n",
    "            out_ = out_.lower()\n",
    "\n",
    "            out_ = tknzr.tokenize(out_)\n",
    "\n",
    "            preprocessedSentence =[]\n",
    "\n",
    "            for word in out_:\n",
    "\n",
    "                if word not in nltk.corpus.stopwords.words('english') and len(word) >= 3:\n",
    "\n",
    "                    preprocessedSentence.append(ps.stem(word))\n",
    "\n",
    "            preprocessedText.append(preprocessedSentence)\n",
    "\n",
    "        return preprocessedText\n",
    "\n",
    "    def preprocessingInputData(input):\n",
    "        allText = [i for i in input]\n",
    "        preprocessedText = [[ps.stem(word) for word in tknzr.tokenize(re.sub(r'\\d+', '', re.sub(r\"http\\S+|www.\\S+\", matchingURL, sentence)).lower()) if word not in nltk.corpus.stopwords.words('english') and len(word) >= 3] for sentence in allText]\n",
    "        return preprocessedText\n",
    "\n",
    "    # def preprocessingInputData(input):\n",
    "    #     allText = [i for i in input]\n",
    "    #     preprocessedText = [[ps.stem(word) for word in tknzr.tokenize(re.sub(r'\\d+', '', re.sub(r\"http\\S+|www.\\S+\", lambda match: urlparse(match.group(\n",
    "    #     )).netloc or match.group().split(\"/\")[0], sentence)).lower()) if word not in nltk.corpus.stopwords.words('english') and len(word) >= 3] for sentence in allText]\n",
    "    #     return preprocessedText\n",
    "\n",
    "\n",
    "    def mapFromWordToIdx(input):\n",
    "\n",
    "        wholeText = []\n",
    "\n",
    "        for s in input:\n",
    "            sentence = [2]\n",
    "            for w in s:\n",
    "                if w in text.tokens:\n",
    "                    sentence.append(text.index(w))\n",
    "                else:\n",
    "                    sentence.append(1)\n",
    "            sentence.append(3)\n",
    "            wholeText.append(sentence)\n",
    "\n",
    "        return wholeText\n",
    "\n",
    "    def CreateDatatset(X, y):\n",
    "\n",
    "        X_len = torch.tensor(list(map(len, X)))\n",
    "        X_len, X_len_idx = X_len.sort(0, descending=True)\n",
    "\n",
    "        X_ordered = [torch.LongTensor(X[i]) for i in X_len_idx]\n",
    "        X_p = pad_sequence(X_ordered, batch_first=True)\n",
    "        y = torch.FloatTensor(np.array(y))\n",
    "        dataset = Data.TensorDataset(X_p, X_len, y)\n",
    "\n",
    "        return dataset\n",
    "    \n",
    "    print(\"Loading Origin Data\")\n",
    "    \n",
    "    if args.full_data:\n",
    "        df = pd.read_html('FullDataFromSQLHSpam14.html')[0].iloc[1:,:]\n",
    "    else:\n",
    "        df_noSPammer = pd.read_html('textMaliciousMark_10th_NotSpammer.html')\n",
    "    #     df_noSPammer = pd.read_html('textMaliciousMark_Small_NotSpammer.html')\n",
    "        df_noSPammer = df_noSPammer[0][1:]\n",
    "        df_Spammer = pd.read_html('textMaliciousMark_10th_Spammer.html')\n",
    "    #     df_Spammer = pd.read_html('textMaliciousMark_Small_Spammer.html')\n",
    "        df_Spammer = df_Spammer[0][1:]\n",
    "        df = pd.concat([df_Spammer, df_noSPammer])\n",
    "        del df_noSPammer, df_Spammer\n",
    "        \n",
    "    tknzr = TweetTokenizer()\n",
    "    ps = nltk.stem.PorterStemmer()\n",
    "    \n",
    "    print(\"Data Splitation\")\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        df[0], df[1], test_size=0.2, stratify=df[1], random_state=64)\n",
    "    X_test, X_validation, Y_test, Y_validation = train_test_split(\n",
    "        X_test, Y_test, test_size=0.5, stratify=Y_test, random_state=64)\n",
    "    \n",
    "    print(\"Preprocessing X_train\")\n",
    "    \n",
    "    X_train = preprocessingInputData(X_train)\n",
    "    \n",
    "    print(\"Generating text\")\n",
    "        \n",
    "    # Preparing the dictionary\n",
    "    text = nltk.Text(list(itertools.chain(*X_train)))\n",
    "    len(dict(text.vocab()))  # number of the vocab\n",
    "    text.tokens  # this is the idx -> word\n",
    "    text.index(\"t.co\")  # this. is the word -> idx\n",
    "    \n",
    "    ## Setting up the vocab size\n",
    "    args.vocab_size = args.vocab_size or len(text.tokens)\n",
    "    \n",
    "    if args.vocab_size:\n",
    "        text.tokens = specialTokenList + [w for w, _ in text.vocab().most_common(args.vocab_size - len(specialTokenList))]\n",
    "    else:\n",
    "        text.tokens = specialTokenList + text.tokens\n",
    "        \n",
    "    args.vocab_size = len(text.tokens)\n",
    "    \n",
    "    print(\"Generating Datasets\")\n",
    "    \n",
    "    training_dataset = CreateDatatset(\n",
    "        mapFromWordToIdx(X_train), list(map(int, list(Y_train))))\n",
    "\n",
    "    print(\"Preprocessing X_validation\")\n",
    "    \n",
    "    X_validation = preprocessingInputData(X_validation)\n",
    "\n",
    "    validation_dataset = CreateDatatset(mapFromWordToIdx(\n",
    "        X_validation), list(map(int, list(Y_validation))))\n",
    "    \n",
    "    print(\"Preprocessing X_test\")\n",
    "\n",
    "    X_test = preprocessingInputData(X_test)\n",
    "\n",
    "    test_dataset = CreateDatatset(mapFromWordToIdx(\n",
    "        X_test), list(map(int, list(Y_test))))\n",
    "    \n",
    "    print(\"Dumping Data\")\n",
    "    \n",
    "    with open(args.pickle_name, \"wb\") as fp:   #Pickling\n",
    "        pickle.dump([training_dataset, validation_dataset, test_dataset, text],fp) \n",
    "        print(\"The Pickle Data Dumped\")\n",
    "    \n",
    "else:\n",
    "    print(\"Loading Existing File: \", args.pickle_name)\n",
    "    with open(args.pickle_name, \"rb\") as fp:   # Unpickling    \n",
    "        training_dataset, validation_dataset, test_dataset, text = pickle.load(fp)\n",
    "        args.vocab_size = args.vocab_size or len(text.tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSampler(dataset):\n",
    "    \n",
    "    target = torch.tensor([ label for t, l , label in dataset])\n",
    "    class_sample_count = torch.tensor(\n",
    "        [(target == t).sum() for t in torch.unique(target, sorted=True)])\n",
    "    weight = 1. / class_sample_count.float()\n",
    "    samples_weight = np.array([weight[t.item()] for t in target.byte()])\n",
    "    samples_weight = torch.from_numpy(samples_weight)\n",
    "    samples_weigth = samples_weight.double()\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    \n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbuser/.local/lib/python3.6/site-packages/torch/utils/data/sampler.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.weights = torch.tensor(weights, dtype=torch.double)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:225: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters in this Model:  3804929\n"
     ]
    }
   ],
   "source": [
    "if args.usingWeightRandomSampling:\n",
    "    sampler = getSampler(training_dataset)\n",
    "else:\n",
    "    sampler = None\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    training_dataset, batch_size=args.batch_size, shuffle=False, drop_last=False, sampler = sampler)\n",
    "valid_loader = DataLoader(\n",
    "    validation_dataset, batch_size=args.batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "trainer = Trainer().to(device)\n",
    "\n",
    "print(\"Number of Parameters in this Model: \",trainer.num_all_params())\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(trainer.optim, 2000, gamma=0.85)\n",
    "# trainer.optim.param_groups[0]['lr']=\n",
    "allStep = 0\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_EXECUTION_FAILED",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-99616bc87e53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mstart_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#         trainer.train_step((X, X_len), y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mend_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-80ff60fdeab0>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, input, label)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Temp_Train_Loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-80ff60fdeab0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, label)\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSCL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-80ff60fdeab0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, lengths)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0;31m#             out = self.rnn(out,(self.h0.repeat(1,B,1), self.c0.repeat(1,B,1)))[0][:, -1, :]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             result = _impl(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 179\u001b[0;31m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED"
     ]
    }
   ],
   "source": [
    "while epoch < args.n_epoch:\n",
    "    for i, (X, X_len, y) in enumerate(train_loader):\n",
    "        trainer.train()\n",
    "        \n",
    "        \n",
    "        X, X_len, y = X.to(device), X_len.to(device), y.to(device)\n",
    "        \n",
    "        args.obser = y\n",
    "        \n",
    "        if trainer.optim.param_groups[0]['lr'] >= 0.00001:\n",
    "            scheduler.step()\n",
    "        start_t = time.time()\n",
    "#         trainer.train_step((X, X_len), y)\n",
    "        trainer.train_step(X, y)\n",
    "\n",
    "        end_t = time.time()\n",
    "        allStep += 1\n",
    "        print('| Epoch [%d] | Step [%d] | lr [%.6f] | Loss: [%.4f] | Acc: [%.4f] | Time: %.1fs' %\n",
    "              (epoch, allStep, trainer.optim.param_groups[0]['lr'], trainer.loss.item(), trainer.accuracy.item(),\n",
    "               end_t - start_t))\n",
    "\n",
    "#         if trainer.accuracy.item() > 0.95: # Stop early\n",
    "#             raise StopIteration\n",
    "        if allStep % args.log_freq == 0:\n",
    "            trainer.plot_train_hist('_SSCL')\n",
    "\n",
    "        if allStep % args.val_freq == 0:\n",
    "\n",
    "            for _ in range(args.val_steps):\n",
    "                trainer.eval()\n",
    "                stIdx = np.random.randint(\n",
    "                    0, len(validation_dataset) - args.batch_size)\n",
    "                v_X, v_X_len, v_y = validation_dataset[stIdx: stIdx +\n",
    "                                                       args.batch_size]\n",
    "                v_X, v_X_len, v_y = v_X.to(\n",
    "                    device), v_X_len.to(device), v_y.to(device)\n",
    "                start_t = time.time()\n",
    "#                 trainer.test_step((v_X, v_X_len), v_y)\n",
    "                trainer.test_step(v_X, v_y)\n",
    "                end_t = time.time()\n",
    "                print('| Epoch [%d] | Validation | Step [%d] |  Loss: [%.4f] | Acc: [%.4f] | Time: %.1fs' %\n",
    "                      (epoch, allStep, trainer.loss.item(), trainer.accuracy.item(), end_t - start_t))\n",
    "            trainer.calculateAverage()\n",
    "            clear_output()\n",
    "            print(\"Confusion Matrix: \")\n",
    "            display(pd.DataFrame(args.confusion_matrics[-1]))\n",
    "            trainer.plot_all('_SSCL')\n",
    "            \n",
    "     # After every Epoch, if can be moved\n",
    "\n",
    "    epoch += 1\n",
    "    trainer.model_save(epoch)\n",
    "\n",
    "#     if epoch >= 20:\n",
    "#         raise StopIteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
