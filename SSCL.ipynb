{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5093],\n",
       "        [0.5178],\n",
       "        [0.4629],\n",
       "        [0.4905],\n",
       "        [0.4966]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# For the use of padding\n",
    "from torch.nn.utils.rnn import pad_sequence, pad_packed_sequence, pack_padded_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as Data\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import gensim\n",
    "import os\n",
    "from nltk.corpus import wordnet as wn  # For WordNet\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Import Data\n",
    "# Padding the incomming Data\n",
    "# Clip the Grad of LSTM\n",
    "# Make h0 and C0 trainable\n",
    "\n",
    "device = torch.device('gpu' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class args(object):\n",
    "\n",
    "    # Data\n",
    "\n",
    "    dataset_path = \"\"  # load a dataset and setting\n",
    "    vocab_size = 2000\n",
    "    seq_len = 20\n",
    "\n",
    "    # Arch\n",
    "\n",
    "    usingPretrainedEmbedding = False\n",
    "    if usingPretrainedEmbedding:\n",
    "        embedding_dim = 300\n",
    "    else:\n",
    "        embedding_dim = 500\n",
    "\n",
    "    hidden = 128\n",
    "\n",
    "    # Training params\n",
    "    batch_size = 5\n",
    "    L2 = 0\n",
    "    threshold = 0.5\n",
    "    lr = 1e-3\n",
    "    epochs = 3\n",
    "\n",
    "    # If using Adam\n",
    "    adam_beta1 = 0.9\n",
    "    adam_beta2 = 0.999\n",
    "    adam_weight_decay = 0.01\n",
    "\n",
    "    # Logging the Training\n",
    "    log_freq = 10\n",
    "    model_save_freq = 1\n",
    "    model_name = 'SSCL'\n",
    "    model_path = './' + model_name + '/Model/'\n",
    "    log_path = './' + model_name + '/Log/'\n",
    "\n",
    "\n",
    "# Create the path for saving model and the log\n",
    "if not os.path.exists(args.model_path):\n",
    "    os.makedirs(args.model_path)\n",
    "\n",
    "if not os.path.exists(args.log_path):\n",
    "    os.makedirs(args.log_path)\n",
    "\n",
    "\n",
    "class Constants():\n",
    "    ''' The Constants for the text '''\n",
    "    PAD = 1\n",
    "    UNK = 0\n",
    "    SOS = 2\n",
    "    EOS = 3\n",
    "\n",
    "    PAD_WORD = '<blank>'\n",
    "    UNK_WORD = '<UNK>'\n",
    "    SOS_WORD = '<SOS>'\n",
    "    EOS_WORD = '<EOS>'\n",
    "\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "# model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "#     './GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "# word2vector = torch.FloatTensor(model.vectors)\n",
    "\n",
    "\n",
    "'''\n",
    "# import the dataset\n",
    "# Preprocessing the data\n",
    "# Split the dataset to (traninig, test, validation)\n",
    "'''\n",
    "\n",
    "\n",
    "class SSCL(nn.Module):\n",
    "\n",
    "    ''' The Model from paper '''\n",
    "\n",
    "    def __init__(self,):\n",
    "        super(SSCL, self).__init__()\n",
    "\n",
    "        if args.usingPretrainedEmbedding:\n",
    "            self.embed = nn.Embedding.from_pretrained(word2vector)\n",
    "        else:\n",
    "            self.embed = nn.Embedding(\n",
    "                args.vocab_size, args.embedding_dim, Constants.PAD)\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(args.embedding_dim, 64, 3, 1, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.rnn = nn.LSTM(64, args.hidden, batch_first=True)\n",
    "\n",
    "        self.out_net = nn.Sequential(\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.h0 = nn.Parameter(torch.randn(1, args.hidden))\n",
    "        self.c0 = nn.Parameter(torch.randn(1, args.hidden))\n",
    "\n",
    "        self.apply(self.weight_init)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        B = input.size(0)\n",
    "        \n",
    "        emb_out = self.embed(input).transpose(1, 2)\n",
    "\n",
    "        out = self.cnn(emb_out).transpose(1, 2)\n",
    "\n",
    "        out = self.rnn(out,(self.h0.repeat(1,B,1), self.c0.repeat(1,B,1)))[0][:, -1, :]\n",
    "\n",
    "        out = self.out_net(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def weight_init(self, m):\n",
    "\n",
    "        if type(m) in [nn.Conv2d, nn.ConvTranspose2d, nn.Linear]:\n",
    "            nn.init.kaiming_normal_(m.weight, 0.2, nonlinearity='leaky_relu')\n",
    "        elif type(m) in [nn.LSTM]:\n",
    "            for name, value in m.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_normal_(value.data)\n",
    "                if 'bias'in name:\n",
    "                    value.data.normal_()\n",
    "\n",
    "\n",
    "# We can put all the lost function and the optimization into the args\n",
    "\n",
    "class Trainer(nn.Module):\n",
    "\n",
    "    def __init__(self,):\n",
    "        super(Trainer, self).__init__()\n",
    "        self.SSCL = SSCL()\n",
    "        self.optim = optim.Adagrad(\n",
    "            self.SSCL.parameters(), lr=args.lr, weight_decay=args.L2)\n",
    "        self.Loss = nn.BCELoss()\n",
    "        self.hist = defaultdict(list)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, input, label):\n",
    "\n",
    "        self.pred = self.SSCL(input)\n",
    "\n",
    "        loss = self.Loss(self.pred, label)\n",
    "\n",
    "        accuracy = torch.mean(\n",
    "            ((self.pred > args.threshold) == label.byte()).float())\n",
    "\n",
    "        return loss, accuracy\n",
    "\n",
    "    def train_step(self, input, label):\n",
    "\n",
    "        self.optim.zero_grad()\n",
    "\n",
    "        loss, accuracy = self.forward(input, label)\n",
    "\n",
    "        self.train_hist[\"Loss\"].append(loss.item())\n",
    "        self.train_hist[\"Accuracy\"].append(accuracy.item())\n",
    "\n",
    "        self.loss.backward()\n",
    "        self.optim.step()\n",
    "\n",
    "    def test_step(self, input, label, validation=True):\n",
    "\n",
    "        # Not Updating the weight\n",
    "\n",
    "        loss, accuracy = self.forward(input, label)\n",
    "\n",
    "        if validation:\n",
    "            self.train_hist[\"V_Loss\"].append(loss.item())\n",
    "            self.train_hist[\"V_Accuracy\"].append(accuracy.item())\n",
    "        else:\n",
    "            self.train_hist[\"T_Loss\"].append(loss.item())\n",
    "            self.train_hist[\"T_Accuracy\"].append(accuracy.item())\n",
    "\n",
    "# input can be = (B, L)\n",
    "\n",
    "\n",
    "test_input = torch.randint(0, args.vocab_size, (args.batch_size, 15))\n",
    "\n",
    "test_lable = torch.ones(args.batch_size, 1)\n",
    "\n",
    "T = Trainer()\n",
    "\n",
    "T(test_input, test_lable)\n",
    "\n",
    "SSCL()(test_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
